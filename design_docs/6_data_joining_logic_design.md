# ðŸ”— Data Joining Logic Design Document

## Executive Summary

This document provides complete specifications for joining the 4 parquet tables (offense_embedding, defense_embedding, game_state_embedding, play_targets) with proper temporal consistency, data integrity validation, and hierarchical organization optimized for the CFB hierarchical model training pipeline.

**ðŸŽ¯ Key Design Goals:**
- **Clean Inner Joins**: Leverage identical play_id coverage across all 4 tables
- **Hierarchical Preservation**: Maintain game_id â†’ drive_id â†’ play_id relationships
- **Football Rules Validation**: YAML-based constraint system for realistic simulation
- **Memory-Efficient Loading**: Year-chunk processing optimized for TPU v2-8 (512GB HBM)
- **4-Container Output**: [offense_embedding], [defense_embedding], [play_embedding], [game_state_embedding]
- **Temporal Consistency**: Validate game state progression and logical relationships
- **Simulation-Ready**: Data quality validation for accurate play-by-play simulation

---

## ðŸ—ï¸ Architecture Overview

### Data Flow Pipeline

```
Raw Parquet Tables (4 sources)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Year-Chunk Memory Loading (TPU Optimized)     â”‚
â”‚  â€¢ Load full year into 512GB HBM               â”‚
â”‚  â€¢ Pre-sorted by play_id for fast joins        â”‚
â”‚  â€¢ Memory-efficient chunk processing           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Inner Join Engine (play_id primary key)       â”‚
â”‚  â€¢ offense_embedding â‹ˆ defense_embedding       â”‚
â”‚  â€¢ â‹ˆ game_state_embedding â‹ˆ play_targets       â”‚
â”‚  â€¢ Preserve hierarchical game_id groupings     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Football Rules Validation (YAML constraints)  â”‚
â”‚  â€¢ 4 downs + administrative plays              â”‚
â”‚  â€¢ Scoring rules (TD=6, FG=3, XP=1)            â”‚
â”‚  â€¢ Clock management (900s quarters, 2-min)     â”‚
â”‚  â€¢ 10 yards for first down                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Integrity Validation Engine              â”‚
â”‚  â€¢ Temporal consistency (scores, time, downs)  â”‚
â”‚  â€¢ Cross-table relationship validation         â”‚
â”‚  â€¢ Missing data handling with smart defaults   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
4-Container Embedding Structure
[offense_embedding] [defense_embedding] [play_embedding] [game_state_embedding]
    â†“
Sequential Batching System (Document #3)
```

---

## ðŸ“‹ Football Rules Configuration

### Core Rules YAML File

```yaml
# cfb_football_rules.yaml
# Basic college football rules for model constraint validation

game_structure:
  quarters: 4
  quarter_duration_seconds: 900  # 15 minutes
  overtime_periods: unlimited
  downs_per_possession: 4
  yards_for_first_down: 10
  field_length_yards: 120  # Including endzones
  field_width_yards: 53

scoring_system:
  touchdown: 6
  field_goal: 3
  extra_point: 1
  two_point_conversion: 2
  safety: 2

clock_management:
  two_minute_warning: 120  # seconds in quarters 2 and 4
  clock_stops_incomplete_pass: true
  clock_stops_out_of_bounds: true
  clock_stops_first_down: true  # College football specific
  clock_stops_penalty: true
  timeout_duration_seconds: 90

possession_rules:
  max_consecutive_downs: 4
  first_down_reset_distance: 10
  turnover_possession_change: true
  scoring_possession_change: true  # Kickoff after scores
  punt_possession_change: true
  failed_fourth_down_change: true

administrative_plays:
  - timeout
  - penalty
  - measurement
  - injury_timeout
  - tv_timeout
  - end_quarter
  - end_half
  - end_game

valid_play_outcomes:
  rushing:
    - gain_yards
    - no_gain
    - loss_yards
    - touchdown
    - fumble
    - first_down
  passing:
    - completion
    - incomplete
    - interception
    - sack
    - touchdown
    - first_down
  kicking:
    - field_goal_good
    - field_goal_missed
    - extra_point_good
    - extra_point_missed
    - punt
  special:
    - kickoff
    - safety
    - blocked_kick
    - return_touchdown

field_position:
  own_endzone: [1, 10]
  own_territory: [11, 50]
  midfield: [45, 55]
  opponent_territory: [51, 80]
  red_zone: [81, 100]
  goal_line: [96, 100]

game_situations:
  garbage_time_threshold: 21  # Point differential in 4th quarter
  two_minute_warning_critical: true
  fourth_down_decision_critical: true
  red_zone_critical: true
  goal_line_critical: true

validation_rules:
  score_progression:
    min_change: 0
    max_change: 8  # TD + 2pt conversion
    valid_increments: [0, 1, 2, 3, 6, 7, 8]
  
  time_progression:
    must_decrease: true  # Time always moves forward
    min_play_duration: 0  # Timeouts can be 0 seconds
    max_play_duration: 15  # 15 seconds (very long play with laterals)
  
  down_progression:
    valid_downs: [1, 2, 3, 4]
    first_down_reset_conditions:
      - gain_10_yards
      - penalty_automatic_first
      - touchdown
      - turnover  # Opponent gets 1st down
  
  field_position:
    valid_range: [1, 100]  # Yard lines
    endzone_scoring: [96, 100]  # Touchdowns only possible here
    
  possession_consistency:
    offense_defense_opposite: true  # One team offense, other defense
    score_attribution_correct: true  # Points go to correct team
```

---

## ðŸ”§ Core Data Joining Implementation

### Main Data Joiner Class

```python
import pandas as pd
import numpy as np
import yaml
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import logging
from datetime import datetime
import gc

class CFBDataJoiner:
    """
    Advanced data joining system for CFB hierarchical model training
    """
    
    def __init__(self, 
                 base_path: str = "/content/drive/MyDrive/cfb_model/parquet_files/",
                 rules_config_path: str = "cfb_football_rules.yaml",
                 memory_limit_gb: float = 400.0,  # Use 400GB of 512GB HBM
                 validation_level: str = "comprehensive"):
        
        self.base_path = Path(base_path)
        self.memory_limit_gb = memory_limit_gb
        self.validation_level = validation_level
        
        # Load football rules
        self.football_rules = self._load_football_rules(rules_config_path)
        
        # Initialize validation systems
        self.validation_results = {}
        self.data_quality_metrics = {}
        
        # Configure logging
        self._setup_logging()
        
        # Track join statistics
        self.join_stats = {
            'total_plays_processed': 0,
            'successful_joins': 0,
            'validation_failures': 0,
            'missing_data_fixes': 0
        }
    
    def _load_football_rules(self, config_path: str) -> Dict[str, Any]:
        """Load football rules from YAML configuration"""
        try:
            with open(config_path, 'r') as f:
                rules = yaml.safe_load(f)
            self.logger.info(f"âœ… Loaded football rules from {config_path}")
            return rules
        except FileNotFoundError:
            # Create default rules if file doesn't exist
            default_rules = self._create_default_football_rules()
            with open(config_path, 'w') as f:
                yaml.dump(default_rules, f, default_flow_style=False)
            self.logger.warning(f"âš ï¸ Created default rules file: {config_path}")
            return default_rules
    
    def _create_default_football_rules(self) -> Dict[str, Any]:
        """Create default football rules dictionary"""
        return {
            'game_structure': {
                'quarters': 4, 'quarter_duration_seconds': 900,
                'downs_per_possession': 4, 'yards_for_first_down': 10
            },
            'scoring_system': {
                'touchdown': 6, 'field_goal': 3, 'extra_point': 1,
                'two_point_conversion': 2, 'safety': 2
            },
            'clock_management': {
                'two_minute_warning': 120,
                'clock_stops_incomplete_pass': True
            },
            'validation_rules': {
                'score_progression': {'min_change': 0, 'max_change': 8},
                'time_progression': {'must_decrease': True},
                'down_progression': {'valid_downs': [1, 2, 3, 4]}
            }
        }
    
    def _setup_logging(self):
        """Setup comprehensive logging system"""
        self.logger = logging.getLogger('CFBDataJoiner')
        self.logger.setLevel(logging.INFO)
        
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
    
    def join_yearly_data(self, years: List[int], 
                        weeks: Optional[List[int]] = None) -> Dict[str, np.ndarray]:
        """
        Main entry point for joining yearly data with comprehensive validation
        
        Args:
            years: List of years to process
            weeks: Optional list of weeks (default: all weeks 1-17)
            
        Returns:
            4-container embedding structure
        """
        self.logger.info(f"ðŸš€ Starting data joining for years: {years}")
        
        # Initialize combined containers
        combined_containers = {
            'offense_embedding': [],
            'defense_embedding': [],
            'play_embedding': [],      # Renamed from play_targets
            'game_state_embedding': []
        }
        
        metadata = {
            'game_hierarchies': [],  # Track game_id -> drive_id -> play_id relationships
            'temporal_sequences': [], # Track temporal ordering
            'validation_reports': []  # Track validation results
        }
        
        # Process each year in memory-efficient chunks
        for year in years:
            self.logger.info(f"ðŸ“‚ Processing year {year}...")
            
            # Load full year into memory (leveraging 512GB HBM)
            yearly_containers, yearly_metadata = self._process_yearly_chunk(year, weeks)
            
            # Append to combined results
            for container_name, container_data in yearly_containers.items():
                combined_containers[container_name].append(container_data)
            
            # Append metadata
            metadata['game_hierarchies'].extend(yearly_metadata['game_hierarchies'])
            metadata['temporal_sequences'].extend(yearly_metadata['temporal_sequences'])
            metadata['validation_reports'].append(yearly_metadata['validation_report'])
            
            # Memory cleanup
            del yearly_containers, yearly_metadata
            gc.collect()
        
        # Concatenate all years
        final_containers = self._concatenate_yearly_results(combined_containers)
        
        # Final validation and optimization
        validated_containers = self._final_validation_and_optimization(
            final_containers, metadata
        )
        
        self._log_join_summary()
        
        return validated_containers
    
    def _process_yearly_chunk(self, year: int, 
                             weeks: Optional[List[int]]) -> Tuple[Dict, Dict]:
        """Process a full year of data in memory"""
        
        if weeks is None:
            weeks = list(range(1, 18))  # Standard CFB weeks 1-17
        
        # Load all 4 tables for the entire year
        yearly_tables = self._load_yearly_tables(year, weeks)
        
        if not yearly_tables:
            self.logger.warning(f"âš ï¸ No data found for year {year}")
            return {}, {}
        
        # Perform inner joins on play_id
        joined_df = self._perform_inner_joins(yearly_tables)
        
        # Validate football rules and data integrity
        validated_df = self._validate_joined_data(joined_df, year)
        
        # Create hierarchical metadata
        hierarchy_metadata = self._extract_hierarchical_metadata(validated_df)
        
        # Convert to 4-container embedding structure
        containers = self._convert_to_embedding_containers(validated_df)
        
        metadata = {
            'game_hierarchies': hierarchy_metadata,
            'temporal_sequences': self._extract_temporal_sequences(validated_df),
            'validation_report': self.validation_results.get(year, {})
        }
        
        return containers, metadata
    
    def _load_yearly_tables(self, year: int, weeks: List[int]) -> Dict[str, pd.DataFrame]:
        """Load all 4 tables for a specific year"""
        
        yearly_tables = {
            'offense_embedding': [],
            'defense_embedding': [],
            'game_state_embedding': [],
            'play_targets': []
        }
        
        # Load each week and accumulate
        for week in weeks:
            week_tables = self._load_week_tables(year, week)
            
            if week_tables:
                for table_name, df in week_tables.items():
                    yearly_tables[table_name].append(df)
        
        # Concatenate weekly data for each table
        final_tables = {}
        for table_name, weekly_dfs in yearly_tables.items():
            if weekly_dfs:
                combined_df = pd.concat(weekly_dfs, ignore_index=True)
                # Sort by play_id for efficient joining
                combined_df = combined_df.sort_values('play_id').reset_index(drop=True)
                final_tables[table_name] = combined_df
                
                self.logger.info(f"ðŸ“Š Loaded {len(combined_df):,} plays from {table_name} for {year}")
        
        return final_tables
    
    def _load_week_tables(self, year: int, week: int) -> Optional[Dict[str, pd.DataFrame]]:
        """Load all 4 tables for a specific week"""
        
        table_paths = {
            'offense_embedding': self.base_path / f"offense_embedding/{year}/week_{week}.parquet",
            'defense_embedding': self.base_path / f"defense_embedding/{year}/week_{week}.parquet",
            'game_state_embedding': self.base_path / f"game_state_embedding/{year}/week_{week}.parquet",
            'play_targets': self.base_path / f"play_targets/{year}/week_{week}.parquet"
        }
        
        # Check if all files exist
        missing_files = [name for name, path in table_paths.items() if not path.exists()]
        if missing_files:
            self.logger.warning(f"âš ï¸ Missing files for {year} week {week}: {missing_files}")
            return None
        
        # Load all tables
        tables = {}
        for table_name, path in table_paths.items():
            try:
                df = pd.read_parquet(path)
                tables[table_name] = df
            except Exception as e:
                self.logger.error(f"âŒ Error loading {path}: {e}")
                return None
        
        return tables
    
    def _perform_inner_joins(self, tables: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """Perform inner joins across all 4 tables on play_id"""
        
        self.logger.info("ðŸ”— Performing inner joins on play_id...")
        
        # Start with offense_embedding as base
        joined_df = tables['offense_embedding'].copy()
        initial_count = len(joined_df)
        
        # Join each table sequentially
        join_order = ['defense_embedding', 'game_state_embedding', 'play_targets']
        
        for table_name in join_order:
            before_count = len(joined_df)
            
            joined_df = joined_df.merge(
                tables[table_name],
                on='play_id',
                how='inner',  # User specified: tables contain identical plays
                suffixes=('', f'_{table_name}')
            )
            
            after_count = len(joined_df)
            
            self.logger.info(f"âœ… Joined {table_name}: {before_count:,} â†’ {after_count:,} plays")
        
        # Clean up duplicate columns
        joined_df = self._clean_duplicate_columns(joined_df)
        
        self.join_stats['total_plays_processed'] += initial_count
        self.join_stats['successful_joins'] += len(joined_df)
        
        self.logger.info(f"ðŸŽ¯ Final joined dataset: {len(joined_df):,} plays")
        
        return joined_df
    
    def _clean_duplicate_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """Remove duplicate columns from joins while preserving hierarchical keys"""
        
        # Preserve key hierarchical columns
        preserve_columns = ['play_id', 'game_id', 'drive_id', 'driveId']
        
        # Find duplicate columns (excluding preserved ones)
        duplicate_patterns = ['year', 'week', 'created_at', 'updated_at', 'season']
        columns_to_drop = []
        
        for pattern in duplicate_patterns:
            duplicate_cols = [
                col for col in df.columns 
                if pattern in col.lower() and '_' in col and col not in preserve_columns
            ]
            columns_to_drop.extend(duplicate_cols)
        
        # Handle game_id duplicates (keep the first, cleanest version)
        game_id_cols = [col for col in df.columns if 'game_id' in col and col != 'game_id']
        columns_to_drop.extend(game_id_cols)
        
        # Drop duplicate columns
        df = df.drop(columns=columns_to_drop, errors='ignore')
        
        self.logger.info(f"ðŸ§¹ Removed {len(columns_to_drop)} duplicate columns")
        
        return df
    
    def _validate_joined_data(self, df: pd.DataFrame, year: int) -> pd.DataFrame:
        """Comprehensive data validation using football rules"""
        
        self.logger.info(f"ðŸ” Validating joined data for {year} using football rules...")
        
        validation_results = {
            'temporal_consistency': self._validate_temporal_consistency(df),
            'scoring_consistency': self._validate_scoring_rules(df),
            'down_progression': self._validate_down_progression(df),
            'field_position': self._validate_field_position(df),
            'game_state_logic': self._validate_game_state_logic(df),
            'missing_data_fixes': self._fix_missing_data(df)
        }
        
        # Apply fixes and log results
        validated_df = validation_results['missing_data_fixes']['fixed_df']
        
        # Store validation results
        self.validation_results[year] = validation_results
        
        # Log validation summary
        total_violations = sum([
            len(result.get('violations', [])) 
            for result in validation_results.values() 
            if isinstance(result, dict) and 'violations' in result
        ])
        
        self.logger.info(f"âœ… Validation complete: {total_violations} violations found and fixed")
        
        return validated_df
    
    def _validate_temporal_consistency(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Validate temporal consistency within games"""
        
        violations = []
        
        # Group by game_id for temporal validation
        for game_id, game_df in df.groupby('game_id'):
            game_df = game_df.sort_values(['period', 'total_seconds_remaining'], 
                                        ascending=[True, False])
            
            # Check time progression
            prev_period = 0
            prev_seconds = float('inf')
            
            for idx, row in game_df.iterrows():
                current_period = row.get('period', 1)
                current_seconds = row.get('total_seconds_remaining', 0)
                
                # Time should decrease or stay same within period
                if current_period == prev_period and current_seconds > prev_seconds + 30:
                    violations.append({
                        'game_id': game_id,
                        'play_id': row['play_id'],
                        'violation': 'time_regression',
                        'expected': f'<= {prev_seconds}',
                        'actual': current_seconds
                    })
                
                # Check period progression
                if current_period < prev_period:
                    violations.append({
                        'game_id': game_id,
                        'play_id': row['play_id'],
                        'violation': 'period_regression',
                        'expected': f'>= {prev_period}',
                        'actual': current_period
                    })
                
                prev_period = current_period
                prev_seconds = current_seconds
        
        return {
            'violations': violations,
            'total_violations': len(violations),
            'games_checked': df['game_id'].nunique()
        }
    
    def _validate_scoring_rules(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Validate scoring against football rules"""
        
        violations = []
        rules = self.football_rules['scoring_system']
        
        for game_id, game_df in df.groupby('game_id'):
            game_df = game_df.sort_values('play_id')
            
            prev_off_score = 0
            prev_def_score = 0
            
            for idx, row in game_df.iterrows():
                off_score = row.get('offense_score', 0)
                def_score = row.get('defense_score', 0)
                
                # Check scoring increments
                off_change = off_score - prev_off_score
                def_change = def_score - prev_def_score
                total_change = off_change + def_change
                
                # Valid scoring increments
                valid_increments = [0, 1, 2, 3, 6]  # From YAML rules
                
                if total_change not in valid_increments:
                    violations.append({
                        'game_id': game_id,
                        'play_id': row['play_id'],
                        'violation': 'invalid_scoring_increment',
                        'change': total_change,
                        'valid_options': valid_increments
                    })
                
                prev_off_score = off_score
                prev_def_score = def_score
        
        return {
            'violations': violations,
            'total_violations': len(violations)
        }
    
    def _validate_down_progression(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Validate down and distance progression"""
        
        violations = []
        
        for drive_id, drive_df in df.groupby(['game_id', 'driveId']):
            drive_df = drive_df.sort_values('play_id')
            
            prev_down = 1
            prev_distance = 10
            
            for idx, row in drive_df.iterrows():
                current_down = row.get('down', 1)
                current_distance = row.get('distance', 10)
                yards_gained = row.get('yardsGained', 0)
                
                # Check for first down reset
                if yards_gained >= prev_distance:
                    # Should reset to 1st and 10
                    if current_down != 1 or current_distance != 10:
                        violations.append({
                            'game_id': drive_id[0],
                            'drive_id': drive_id[1], 
                            'play_id': row['play_id'],
                            'violation': 'first_down_not_reset',
                            'yards_gained': yards_gained,
                            'prev_distance': prev_distance,
                            'current_down': current_down,
                            'current_distance': current_distance
                        })
                
                # Check valid down progression
                valid_downs = [-1, 0, 1, 2, 3, 4]  # Include administrative downs from database analysis
                if current_down not in valid_downs:
                    violations.append({
                        'game_id': drive_id[0],
                        'play_id': row['play_id'],
                        'violation': 'invalid_down',
                        'down': current_down,
                        'valid_downs': valid_downs
                    })
                
                prev_down = current_down
                prev_distance = current_distance
        
        return {
            'violations': violations,
            'total_violations': len(violations)
        }
    
    def _validate_field_position(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Validate field position consistency"""
        
        violations = []
        
        for idx, row in df.iterrows():
            yardline = row.get('yardline', 50)
            yards_to_goal = row.get('yards_to_goal', 50)
            
            # Basic field position validation (expanded for behind-goal-line plays)
            if not (-10 <= yardline <= 110):
                violations.append({
                    'play_id': row['play_id'],
                    'violation': 'invalid_yardline',
                    'yardline': yardline,
                    'valid_range': '[-10, 110]'
                })
            
            if not (-10 <= yards_to_goal <= 110):
                violations.append({
                    'play_id': row['play_id'],
                    'violation': 'invalid_yards_to_goal',
                    'yards_to_goal': yards_to_goal,
                    'valid_range': '[-10, 110]'
                })
            
            # Check yardline-to-goal consistency
            expected_ytg = 100 - yardline
            if abs(yards_to_goal - expected_ytg) > 5:  # 5-yard tolerance
                violations.append({
                    'play_id': row['play_id'],
                    'violation': 'yardline_ytg_mismatch',
                    'yardline': yardline,
                    'yards_to_goal': yards_to_goal,
                    'expected_ytg': expected_ytg
                })
        
        return {
            'violations': violations,
            'total_violations': len(violations)
        }
    
    def _validate_game_state_logic(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Validate game state logical consistency"""
        
        violations = []
        
        for idx, row in df.iterrows():
            # Red zone validation
            yards_to_goal = row.get('yards_to_goal', 50)
            is_red_zone = row.get('is_red_zone', 0)
            is_goal_line = row.get('is_goal_line', 0)
            
            # Red zone should be <= 20 yards
            if (yards_to_goal <= 20) != bool(is_red_zone):
                violations.append({
                    'play_id': row['play_id'],
                    'violation': 'red_zone_flag_mismatch',
                    'yards_to_goal': yards_to_goal,
                    'is_red_zone': is_red_zone
                })
            
            # Goal line should be <= 5 yards
            if (yards_to_goal <= 5) != bool(is_goal_line):
                violations.append({
                    'play_id': row['play_id'],
                    'violation': 'goal_line_flag_mismatch',
                    'yards_to_goal': yards_to_goal,
                    'is_goal_line': is_goal_line
                })
            
            # Two minute warning validation
            period = row.get('period', 1)
            seconds_remaining = row.get('total_seconds_remaining', 900)
            is_two_minute = row.get('is_two_minute_warning', 0)
            
            # Should be true in quarters 2,4 with <= 120 seconds OR any OT
            expected_two_minute = (
                (period in [2, 4] and seconds_remaining <= 120) or 
                (period >= 5)
            )
            
            if expected_two_minute != bool(is_two_minute):
                violations.append({
                    'play_id': row['play_id'],
                    'violation': 'two_minute_warning_mismatch',
                    'period': period,
                    'seconds_remaining': seconds_remaining,
                    'is_two_minute': is_two_minute,
                    'expected': expected_two_minute
                })
        
        return {
            'violations': violations,
            'total_violations': len(violations)
        }
    
    def _fix_missing_data(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Fix missing data with smart defaults"""
        
        df_fixed = df.copy()
        fixes_applied = []
        
        # Key columns that should not be missing
        key_columns = {
            'down': 1,
            'distance': 10,
            'period': 1,
            'total_seconds_remaining': 900,
            'offense_score': 0,
            'defense_score': 0,
            'yardline': 50,
            'yards_to_goal': 50
        }
        
        for column, default_value in key_columns.items():
            if column in df_fixed.columns:
                missing_count = df_fixed[column].isna().sum()
                if missing_count > 0:
                    df_fixed[column] = df_fixed[column].fillna(default_value)
                    fixes_applied.append({
                        'column': column,
                        'missing_count': missing_count,
                        'default_value': default_value
                    })
        
        # Forward fill within games for sequential data
        sequential_columns = ['offense_score', 'defense_score', 'period', 'total_seconds_remaining']
        
        for game_id, group in df_fixed.groupby('game_id'):
            for col in sequential_columns:
                if col in df_fixed.columns:
                    # Forward fill within game
                    df_fixed.loc[group.index, col] = group[col].fillna(method='ffill')
                    # Backward fill for remaining missing values
                    df_fixed.loc[group.index, col] = group[col].fillna(method='bfill')
        
        self.join_stats['missing_data_fixes'] += len(fixes_applied)
        
        return {
            'fixed_df': df_fixed,
            'fixes_applied': fixes_applied,
            'total_fixes': len(fixes_applied)
        }
    
    def _extract_hierarchical_metadata(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """Extract hierarchical game â†’ drive â†’ play relationships"""
        
        hierarchies = []
        
        for game_id, game_df in df.groupby('game_id'):
            game_hierarchy = {
                'game_id': game_id,
                'total_plays': len(game_df),
                'drives': {}
            }
            
            for drive_id, drive_df in game_df.groupby('driveId'):
                drive_hierarchy = {
                    'drive_id': drive_id,
                    'play_count': len(drive_df),
                    'play_ids': drive_df['play_id'].tolist(),
                    'start_yardline': drive_df.iloc[0].get('drive_start_yardline', 25),
                    'total_yards': drive_df['yardsGained'].sum() if 'yardsGained' in drive_df else 0
                }
                
                game_hierarchy['drives'][drive_id] = drive_hierarchy
            
            game_hierarchy['drive_count'] = len(game_hierarchy['drives'])
            hierarchies.append(game_hierarchy)
        
        return hierarchies
    
    def _extract_temporal_sequences(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """Extract temporal sequence information for training"""
        
        sequences = []
        
        for game_id, game_df in df.groupby('game_id'):
            # Sort by temporal order
            game_df = game_df.sort_values(['period', 'total_seconds_remaining'], 
                                        ascending=[True, False])
            
            sequence_info = {
                'game_id': game_id,
                'play_sequence': game_df['play_id'].tolist(),
                'temporal_features': {
                    'periods': game_df['period'].tolist(),
                    'time_remaining': game_df['total_seconds_remaining'].tolist(),
                    'score_progression': list(zip(
                        game_df['offense_score'].tolist(),
                        game_df['defense_score'].tolist()
                    ))
                }
            }
            
            sequences.append(sequence_info)
        
        return sequences
    
    def _convert_to_embedding_containers(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:
        """Convert joined DataFrame to 4-container embedding structure"""
        
        self.logger.info("ðŸ“¦ Converting to 4-container embedding structure...")
        
        containers = {}
        
        # 1. OFFENSE EMBEDDING CONTAINER
        offense_categorical = {
            'offense_conference': self._encode_categorical(df['offense_conference']),
            'coach_offense': self._encode_categorical(
                df['coach_first_name'].fillna('') + '_' + df['coach_last_name'].fillna('')
            ),
            'home_away': df['home_away_indicator'].fillna(0).values,
            'new_coach': df['new_coach_indicator'].fillna(0).values
        }
        
        offense_numerical_cols = [
            'years_at_school', 'coach_total_experience', 'talent_zscore',
            'run_rate_1st_down', 'run_rate_2nd_short', 'run_rate_2nd_medium',
            'run_rate_2nd_long', 'run_rate_3rd_short', 'run_rate_3rd_medium',
            'run_rate_3rd_long', 'punt_rate_4th_short', 'punt_rate_4th_medium',
            'punt_rate_4th_long', 'fg_attempt_rate_by_field_position',
            'go_for_it_rate_4th_down', 'go_for_2_rate', 'onside_kick_rate',
            'fake_punt_rate', 'avg_seconds_per_play', 'plays_per_game',
            'penalty_rate', 'penalty_yards_per_game', 'recent_avg_seconds_per_play',
            'recent_plays_per_game', 'recent_penalty_rate',
            'recent_run_rate_by_down_distance', 'opponent_wins', 'opponent_losses',
            'home_wins', 'home_losses', 'away_wins', 'away_losses',
            'conference_wins', 'conference_losses', 'avg_opponent_talent_rating',
            'avg_opponent_talent_rating_of_wins', 'avg_opponent_talent_rating_of_losses',
            'strength_of_schedule', 'wins_vs_favored_opponents',
            'losses_vs_weaker_opponents', 'point_differential_vs_talent_expectation'
        ]
        
        offense_numerical = np.column_stack([
            df[col].fillna(0).values for col in offense_numerical_cols
        ])
        
        containers['offense_embedding'] = {
            **offense_categorical,
            'numerical': offense_numerical
        }
        
        # 2. DEFENSE EMBEDDING CONTAINER
        defense_categorical = {
            'defense_conference': self._encode_categorical(df['defense_conference']),
            'coach_defense': self._encode_categorical(
                df['defense_coach_first_name'].fillna('') + '_' + df['defense_coach_last_name'].fillna('')
            ),
            'defense_new_coach': df['defense_new_coach_indicator'].fillna(0).values
        }
        
        defense_numerical_cols = [
            'defense_years_at_school', 'defense_coach_total_experience',
            'defense_talent_zscore', 'defense_run_stop_rate_1st_down',
            'defense_run_stop_rate_2nd_short', 'defense_run_stop_rate_2nd_medium',
            'defense_run_stop_rate_2nd_long', 'defense_run_stop_rate_3rd_short',
            'defense_run_stop_rate_3rd_medium', 'defense_run_stop_rate_3rd_long',
            'defense_red_zone_fg_rate', 'defense_red_zone_stop_rate',
            'defense_avg_seconds_allowed_per_play', 'defense_plays_allowed_per_game',
            'defense_penalty_rate', 'defense_penalty_yards_per_game',
            'defense_recent_avg_seconds_allowed_per_play',
            'defense_recent_plays_allowed_per_game', 'defense_recent_penalty_rate',
            'defense_recent_stop_rate_by_down_distance', 'defense_opponent_wins',
            'defense_opponent_losses', 'defense_home_wins', 'defense_home_losses',
            'defense_away_wins', 'defense_away_losses', 'defense_conference_wins',
            'defense_conference_losses', 'defense_avg_opponent_talent_rating',
            'defense_avg_opponent_talent_rating_of_wins',
            'defense_avg_opponent_talent_rating_of_losses',
            'defense_strength_of_schedule', 'defense_wins_vs_favored_opponents',
            'defense_losses_vs_weaker_opponents',
            'defense_point_differential_vs_talent_expectation'
        ]
        
        defense_numerical = np.column_stack([
            df[col].fillna(0).values for col in defense_numerical_cols
        ])
        
        containers['defense_embedding'] = {
            **defense_categorical,
            'numerical': defense_numerical
        }
        
        # 3. PLAY EMBEDDING CONTAINER (renamed from play_targets)
        play_categorical = {
            'down': self._encode_categorical(df['down'].fillna(1)),
            'period': self._encode_categorical(df['period'].fillna(1))
        }
        
        # Binary flags for play outcomes
        play_binary_cols = [
            'is_rush', 'is_pass', 'is_punt', 'is_field_goal',
            'is_extra_point', 'is_kickoff', 'is_penalty', 'is_timeout',
            'is_sack', 'is_administrative', 'is_touchdown', 'is_completion',
            'is_interception', 'is_fumble_lost', 'is_fumble_recovered',
            'is_return_td', 'is_safety', 'is_good', 'is_two_point',
            'is_first_down'
        ]
        
        play_numerical_cols = [
            'distance', 'yardline', 'yards_to_goal', 'clock',
            'offense_score', 'defense_score', 'score_differential', 'yardsGained'
        ]
        
        play_binary = np.column_stack([
            df[col].fillna(0).values for col in play_binary_cols
        ])
        
        play_numerical = np.column_stack([
            df[col].fillna(0).values for col in play_numerical_cols
        ])
        
        containers['play_embedding'] = {
            **play_categorical,
            'binary_flags': play_binary,
            'numerical': play_numerical
        }
        
        # 4. GAME STATE EMBEDDING CONTAINER
        game_state_categorical = {
            'venue_id': self._encode_categorical(df['venue_id']),
            'wind_direction': self._encode_wind_direction(df['wind_direction']),
            'game_indoors': df['game_indoors'].fillna(0).values,
            'is_field_turf': df['is_field_turf'].fillna(0).values,
            'is_offense_home_team': df['is_offense_home_team'].fillna(0).values,
            'conference_game': df['conference_game'].fillna(0).values,
            'bowl_game': df['bowl_game'].fillna(0).values
        }
        
        game_state_numerical_cols = [
            'drive_number', 'drive_plays_so_far', 'drive_yards_so_far',
            'drive_start_yardline', 'drive_time_elapsed', 'down', 'distance',
            'yardline', 'yards_to_goal', 'period', 'total_seconds_remaining',
            'offense_score', 'defense_score', 'score_differential',
            'offense_timeouts', 'defense_timeouts', 'temperature', 'humidity',
            'wind_speed', 'precipitation', 'is_red_zone', 'is_goal_line',
            'is_two_minute_warning', 'is_garbage_time', 'is_money_down',
            'is_plus_territory'
        ]
        
        game_state_numerical = np.column_stack([
            df[col].fillna(0).values for col in game_state_numerical_cols
        ])
        
        containers['game_state_embedding'] = {
            **game_state_categorical,
            'numerical': game_state_numerical
        }
        
        self.logger.info("âœ… 4-container embedding structure created successfully")
        
        return containers
    
    def _encode_categorical(self, series: pd.Series) -> np.ndarray:
        """Encode categorical features as integers"""
        unique_values = series.unique()
        value_to_id = {val: idx for idx, val in enumerate(unique_values)}
        return np.array([value_to_id.get(val, 0) for val in series])
    
    def _encode_wind_direction(self, wind_series: pd.Series) -> Dict[str, np.ndarray]:
        """Encode wind direction as circular features"""
        wind_values = wind_series.fillna(0).values
        return {
            'wind_direction_sin': np.sin(wind_values * np.pi / 180),
            'wind_direction_cos': np.cos(wind_values * np.pi / 180)
        }
    
    def _concatenate_yearly_results(self, combined_containers: Dict[str, List]) -> Dict[str, Dict]:
        """Concatenate results across multiple years"""
        
        self.logger.info("ðŸ”„ Concatenating multi-year results...")
        
        final_containers = {}
        
        for container_name, yearly_data in combined_containers.items():
            if not yearly_data:
                continue
                
            final_containers[container_name] = {}
            
            # Get all unique keys across years
            all_keys = set()
            for year_data in yearly_data:
                all_keys.update(year_data.keys())
            
            # Concatenate each key
            for key in all_keys:
                arrays_to_concat = []
                for year_data in yearly_data:
                    if key in year_data:
                        arrays_to_concat.append(year_data[key])
                
                if arrays_to_concat:
                    if isinstance(arrays_to_concat[0], np.ndarray):
                        final_containers[container_name][key] = np.concatenate(arrays_to_concat)
                    else:
                        # Handle other data types (like dictionaries)
                        final_containers[container_name][key] = arrays_to_concat[0]
        
        return final_containers
    
    def _final_validation_and_optimization(self, containers: Dict, metadata: Dict) -> Dict:
        """Final validation and optimization of joined data"""
        
        self.logger.info("ðŸ” Performing final validation and optimization...")
        
        # Validate container consistency
        container_lengths = []
        for container_name, container_data in containers.items():
            for key, data in container_data.items():
                if isinstance(data, np.ndarray) and data.ndim >= 1:
                    container_lengths.append(len(data))
        
        # Ensure all containers have same number of plays
        if len(set(container_lengths)) > 1:
            self.logger.warning(f"âš ï¸ Inconsistent container lengths: {set(container_lengths)}")
        
        # Add metadata to containers
        containers['metadata'] = {
            'total_plays': container_lengths[0] if container_lengths else 0,
            'total_games': len(metadata.get('game_hierarchies', [])),
            'validation_summary': self._summarize_validation_results(),
            'join_statistics': self.join_stats
        }
        
        return containers
    
    def _summarize_validation_results(self) -> Dict[str, Any]:
        """Summarize all validation results"""
        
        total_violations = 0
        validation_summary = {}
        
        for year, year_results in self.validation_results.items():
            year_violations = 0
            for validation_type, result in year_results.items():
                if isinstance(result, dict) and 'total_violations' in result:
                    violations = result['total_violations']
                    year_violations += violations
                    
                    if validation_type not in validation_summary:
                        validation_summary[validation_type] = 0
                    validation_summary[validation_type] += violations
            
            total_violations += year_violations
        
        validation_summary['total_violations_all_years'] = total_violations
        validation_summary['validation_rate'] = (
            1.0 - (total_violations / max(1, self.join_stats['successful_joins']))
        )
        
        return validation_summary
    
    def _log_join_summary(self):
        """Log comprehensive joining summary"""
        
        self.logger.info("=" * 60)
        self.logger.info("ðŸŽ¯ CFB DATA JOINING SUMMARY")
        self.logger.info("=" * 60)
        self.logger.info(f"ðŸ“Š Total plays processed: {self.join_stats['total_plays_processed']:,}")
        self.logger.info(f"âœ… Successful joins: {self.join_stats['successful_joins']:,}")
        self.logger.info(f"âš ï¸ Validation issues: {self.join_stats['validation_failures']:,}")
        self.logger.info(f"ðŸ”§ Missing data fixes: {self.join_stats['missing_data_fixes']:,}")
        
        join_success_rate = (
            self.join_stats['successful_joins'] / 
            max(1, self.join_stats['total_plays_processed'])
        )
        self.logger.info(f"ðŸ“ˆ Join success rate: {join_success_rate:.2%}")
        
        validation_summary = self._summarize_validation_results()
        self.logger.info(f"ðŸ” Data quality rate: {validation_summary.get('validation_rate', 0):.2%}")
        
        self.logger.info("=" * 60)
```

---

## ðŸš€ Usage Example

### Complete Data Joining Pipeline

```python
# Initialize the data joiner
joiner = CFBDataJoiner(
    base_path="/content/drive/MyDrive/cfb_model/parquet_files/",
    rules_config_path="cfb_football_rules.yaml",
    memory_limit_gb=400.0,  # Use 400GB of 512GB TPU HBM
    validation_level="comprehensive"
)

# Join multiple years of data
train_years = [2019, 2020, 2021, 2022]
validation_years = [2023]
test_years = [2024]

# Process training data
print("ðŸš€ Processing training data...")
train_containers = joiner.join_yearly_data(train_years)

# Process validation data  
print("ðŸš€ Processing validation data...")
val_containers = joiner.join_yearly_data(validation_years)

# Process test data
print("ðŸš€ Processing test data...")
test_containers = joiner.join_yearly_data(test_years)

# Verify 4-container structure
print("\nðŸ“¦ Container Structure Verification:")
for split_name, containers in [
    ("train", train_containers),
    ("validation", val_containers), 
    ("test", test_containers)
]:
    print(f"\n{split_name.upper()} SET:")
    for container_name in ['offense_embedding', 'defense_embedding', 'play_embedding', 'game_state_embedding']:
        if container_name in containers:
            container = containers[container_name]
            print(f"  {container_name}:")
            for key, data in container.items():
                if isinstance(data, np.ndarray):
                    print(f"    {key}: {data.shape}")
                else:
                    print(f"    {key}: {type(data)}")

# Access metadata
metadata = train_containers['metadata']
print(f"\nðŸ“Š Training Data Summary:")
print(f"  Total plays: {metadata['total_plays']:,}")
print(f"  Total games: {metadata['total_games']:,}")
print(f"  Data quality rate: {metadata['validation_summary']['validation_rate']:.2%}")
print(f"  Join success rate: {metadata['join_statistics']['successful_joins'] / metadata['join_statistics']['total_plays_processed']:.2%}")

# Save processed data
np.savez_compressed("cfb_train_joined.npz", **train_containers)
np.savez_compressed("cfb_val_joined.npz", **val_containers)
np.savez_compressed("cfb_test_joined.npz", **test_containers)

print("âœ… Data joining pipeline complete!")
```

---

## ðŸŽ¯ Integration with Sequential Batching

### Seamless Integration Design

```python
class JoinedDataToBatchingAdapter:
    """
    Adapter to convert joined 4-container data to sequential batching format
    """
    
    def __init__(self, joined_containers: Dict[str, Dict]):
        self.containers = joined_containers
        self.metadata = joined_containers.get('metadata', {})
    
    def convert_to_sequential_format(self) -> Dict[str, Any]:
        """
        Convert 4-container format to sequential batching input format
        """
        
        # Create the expected format for Document #3 (Sequential Batching)
        sequential_format = {
            'offense_embedding': self.containers['offense_embedding'],
            'defense_embedding': self.containers['defense_embedding'],
            'game_state_embedding': self.containers['game_state_embedding'],
            'play_targets': self.containers['play_embedding'],  # Map play_embedding â†’ play_targets
            'metadata': self.metadata
        }
        
        return sequential_format
    
    def validate_sequential_compatibility(self) -> Dict[str, bool]:
        """Validate compatibility with sequential batching system"""
        
        validation_results = {
            'all_containers_present': all(
                container in self.containers 
                for container in ['offense_embedding', 'defense_embedding', 'play_embedding', 'game_state_embedding']
            ),
            'consistent_play_counts': self._check_consistent_play_counts(),
            'hierarchical_keys_present': self._check_hierarchical_keys(),
            'temporal_ordering_valid': self._validate_temporal_ordering()
        }
        
        return validation_results
    
    def _check_consistent_play_counts(self) -> bool:
        """Check that all containers have consistent play counts"""
        play_counts = []
        
        for container_name in ['offense_embedding', 'defense_embedding', 'play_embedding', 'game_state_embedding']:
            container = self.containers.get(container_name, {})
            for key, data in container.items():
                if isinstance(data, np.ndarray) and data.ndim >= 1:
                    play_counts.append(len(data))
                    break  # Only need one count per container
        
        return len(set(play_counts)) == 1  # All counts should be identical
    
    def _check_hierarchical_keys(self) -> bool:
        """Check that hierarchical keys are preserved"""
        required_hierarchical_info = ['game_hierarchies', 'temporal_sequences']
        metadata = self.containers.get('metadata', {})
        
        return all(key in metadata for key in required_hierarchical_info)
    
    def _validate_temporal_ordering(self) -> bool:
        """Validate temporal ordering for sequential processing"""
        # This would validate that plays are properly ordered for sequential batching
        # Implementation depends on specific temporal requirements
        return True  # Placeholder
```

---

## âœ… Next Steps Integration

This Data Joining Logic design provides:

âœ… **Clean Inner Joins**: Leverages identical play_id coverage across all 4 tables  
âœ… **Football Rules YAML**: Constraint system for realistic simulation and validation  
âœ… **Hierarchical Preservation**: Maintains game_id â†’ drive_id â†’ play_id relationships  
âœ… **Memory-Efficient Loading**: Year-chunk processing for TPU v2-8 optimization  
âœ… **4-Container Output**: [offense_embedding], [defense_embedding], [play_embedding], [game_state_embedding]  
âœ… **Comprehensive Validation**: Temporal consistency, scoring rules, field position logic  
âœ… **Missing Data Handling**: Smart defaults and forward/backward filling strategies  
âœ… **Simulation-Ready**: Data quality validation for accurate game state management  

**Ready for integration with:**
- Sequential Batching System (Document #3) 
- Game State Management System (Document #4)
- Hyperparameter Configuration (Document #5)
- Vegas Data Integration (Document #7)

**ðŸŽ¯ Key Innovation**: The football rules YAML system provides model constraint validation that ensures predictions stay within realistic bounds during training and simulation, while the 4-container structure perfectly aligns with the embedding architecture for efficient hierarchical processing.